defaults:
  - task: metaworld_dial-turn
  - policy: rl100_default
  - _self_

# ============================================
# 训练参数 - v3改进版配置
# ============================================
training:
  seed: 42
  device: cuda:0

  # ----------------------------------------
  # 奖励设置（新增）
  # ----------------------------------------
  # 稀疏奖励模式：只在成功时给予奖励，其他时候为0
  # 适用于想要更难的探索挑战，或者避免距离奖励的干扰
  use_sparse_reward: true      # 是否使用稀疏奖励（默认false使用密集奖励）
  sparse_reward_value: 1.0      # 稀疏奖励的值（成功时给予的奖励）

  # ----------------------------------------
  # Phase 1: IL预训练
  # ----------------------------------------
  il_epochs: 70  # IL预训练轮数

  # ----------------------------------------
  # Phase 2: 离线RL迭代（带Early Stopping）
  # ----------------------------------------
  rl_iterations: 10             # 最多10轮迭代
  rl_epochs_per_iter: 50        # 每轮迭代的训练epochs
  rollout_episodes: 15          # 每轮rollout的episode数
  iql_epochs_per_iter: 10    # IQL预训练的epoch数（可选，默认10）
  # ===== v3优化：减少IL微调，防止catastrophic forgetting =====
  il_finetune_enabled: true     # 启用IL微调
  il_finetune_epochs: 5         # IL自我纠正的epochs（从20减少到5）

  # ===== v3新增：数据质量筛选 =====
  data_quality_threshold: 0.3   # rollout成功率阈值（低于此值的数据将被丢弃）
  max_online_data_ratio: 0.5    # 在线数据占总数据的最大比例（防止在线数据淹没离线数据）

  # ===== v3新增：Early Stopping =====
  early_stop_patience: 3        # 连续N轮评估无提升则提前停止Phase 2

  # ----------------------------------------
  # Phase 3: 在线RL（从最佳checkpoint开始）
  # ----------------------------------------
  online_rl_enabled: true
  online_rl_epochs: 30          # 在线RL的训练epochs
  online_rollout_episodes: 30   # 每个epoch的rollout episodes
  online_replay_size: 5000      # 在线replay buffer大小

  # ===== v3新增：Phase 3学习率调整 =====
  phase3_lr_factor: 0.3         # Phase 3的学习率衰减因子（原学习率 × 0.3，更温和）

  # ===== v3改进：在线数据集数量限制 =====
  max_online_datasets: 5        # 最多保留5个在线数据集（替代比例限制）

  # ----------------------------------------
  # 评估与Checkpoint
  # ----------------------------------------
  eval_freq: 2                  # 每N轮迭代评估一次
  keep_top_k_checkpoints: 5     # 保留Top-K个最佳模型

  # ----------------------------------------
  # 优化细节
  # ----------------------------------------
  target_update_freq: 200       # Target Q网络更新频率（步数）
  grad_clip_norm: 3.0           # 梯度裁剪阈值
  max_datasets: 12              # 最多保留的数据集数量（防止OOM）

  # ----------------------------------------
  # 学习率调度
  # ----------------------------------------
  total_steps: 200000           # 预估总训练步数

  # ----------------------------------------
  # 可选：从checkpoint恢复
  # ----------------------------------------
  resume_path: null

# ============================================
# 优化器配置
# ============================================
optimizer:
  _target_: torch.optim.AdamW
  lr: 5.0e-5              # 基础学习率
  weight_decay: 1.0e-6
  betas: [0.9, 0.999]
  eps: 1.0e-8

# ============================================
# 学习率调度器
# ============================================
lr_scheduler:
  name: cosine
  warmup_steps: 2000      # Warmup步数

# ============================================
# 数据加载器
# ============================================
dataloader:
  batch_size: 64
  num_workers: 4

# ============================================
# 日志配置
# ============================================
logging:
  project: "RL100-MetaWorld-v3"
  name: "rl100_${now:%Y%m%d_%H%M%S}"
  log_freq: 10
  wandb_mode: offline

# ============================================
# v3改进总结
# ============================================
# 核心改进：
# 1. ✅ 数据质量筛选：只保留成功率>30%的rollout数据
# 2. ✅ Early Stopping：连续3轮评估无提升时提前停止Phase 2
# 3. ✅ 在线数据比例限制：在线数据最多占总数据的50%
# 4. ✅ Phase 3从最佳checkpoint开始：避免从性能下降的模型继续训练
# 5. ✅ Phase 3学习率衰减：降低学习率到原来的0.1倍，提高稳定性
# 6. ✅ 更详细的性能追踪：记录性能历史，便于分析
#
# 预期改进效果：
# - 减少低质量数据污染训练集
# - 避免灾难性遗忘和性能退化
# - Phase 3从80%起点继续，而不是从25%起点
# - 更稳定的训练过程
#
# 推荐使用场景：
# - 当Phase 2出现性能退化时
# - 当Phase 3性能远低于Phase 2时
# - 当在线数据质量不稳定时
