Job start at 2025-12-23 12:52:59
Job run at:
   Static hostname: localhost.localdomain
Transient hostname: r8a100-a03
         Icon name: computer-server
           Chassis: server
        Machine ID: 3453da9667674a64b7f3fbb34cc39a16
           Boot ID: 557e318376c64003a5a904e65701ee8c
  Operating System: Rocky Linux 8.7 (Green Obsidian)
       CPE OS Name: cpe:/o:rocky:rocky:8:GA
            Kernel: Linux 4.18.0-425.10.1.el8_7.x86_64
      Architecture: x86-64
Filesystem                                        Size  Used Avail Use% Mounted on
/dev/mapper/rl-root                               120G   25G   96G  21% /
/dev/sdb1                                         1.1T  8.1G  1.1T   1% /tmp
/dev/sdb2                                         4.2T   30G  4.2T   1% /local
/dev/mapper/rl-var                                768G   26G  743G   4% /var
/dev/sda2                                         2.0G  306M  1.7G  15% /boot
/dev/sda1                                         599M  5.8M  594M   1% /boot/efi
ssd.nas00.future.cn:/rocky8_home                   16G   14G  2.2G  87% /home
ssd.nas00.future.cn:/rocky8_workspace             400G     0  400G   0% /workspace
ssd.nas00.future.cn:/rocky8_tools                 5.0T   99G  5.0T   2% /tools
ssd.nas00.future.cn:/centos7_home                  16G  4.1G   12G  26% /centos7/home
ssd.nas00.future.cn:/centos7_workspace            400G     0  400G   0% /centos7/workspace
ssd.nas00.future.cn:/centos7_tools                5.0T  235G  4.8T   5% /centos7/tools
ssd.nas00.future.cn:/eda-tools                    8.0T  5.7T  2.4T  72% /centos7/eda-tools
hdd.nas00.future.cn:/share_personal               500G     0  500G   0% /share/personal
zone05.nas01.future.cn:/NAS_HPC_collab_codemodel   40T   37T  3.8T  91% /share/collab/codemodel
ext-zone00.nas02.future.cn:/nfs_global            379T  378T  607G 100% /nfs_global
ssd.nas00.future.cn:/common_datasets               75T   64T   12T  85% /datasets
Currently Loaded Modulefiles: 1) cluster-tools/v1.0 3) cuda-cudnn/12.1-8.9.3 5) git/2.31.1 2) cmake/3.21.7 4) gcc/9.3.0 6) slurm-tools/v1.0
/tools/cluster-software/gcc/gcc-9.3.0/bin/gcc
/nfs_global/S/yangrongzheng/conda_envs/dp3/bin/python
/nfs_global/S/yangrongzheng/conda_envs/dp3/bin/python3
############### /home : /home/S/yangrongzheng
Disk quotas for user yangrongzheng (uid 6215): 
     Filesystem   space   quota   limit   grace   files   quota   limit   grace
          /home  14173M  16384M  20480M            156k       0       0        

############### /workspace
Disk quotas for user yangrongzheng (uid 6215): 
     Filesystem   space   quota   limit   grace   files   quota   limit   grace
     /workspace      0K    400G    500G               1       0       0        

############### /nfs_global
Disk quotas for user yangrongzheng (uid 6215): 
     Filesystem   space   quota   limit   grace   files   quota   limit   grace
    /nfs_global    107G   5120G   7168G            364k   5000k  10000k        

name, driver_version, power.limit [W]
NVIDIA A100-PCIE-40GB, 570.124.06, 225.00 W
NVIDIA A100-PCIE-40GB, 570.124.06, 225.00 W
NVIDIA A100-PCIE-40GB, 570.124.06, 225.00 W
NVIDIA A100-PCIE-40GB, 570.124.06, 225.00 W
NVIDIA A100-PCIE-40GB, 570.124.06, 225.00 W
NVIDIA A100-PCIE-40GB, 570.124.06, 225.00 W
NVIDIA A100-PCIE-40GB, 570.124.06, 225.00 W
NVIDIA A100-PCIE-40GB, 570.124.06, 225.00 W
Using GPU(s) 0,1,2,3,4,5,6,7
This job is assigned the following resources by SLURM:
CPU_IDs=0-111 GRES=gpu:8(IDX:0-7)
Main program continues to run. Monitoring information will be exported after three hours.
Extracting GPU stats logs using atop has been completed on r8a100-a03.
Logs are being saved to: /nfs_global/S/yangrongzheng/3D-Diffusion-Policy/3D-Diffusion-Policy/atop-719666-r8a100-a03-gpustat.log
