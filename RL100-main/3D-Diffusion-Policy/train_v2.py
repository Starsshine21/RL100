"""
RL100 æ”¹è¿›ç‰ˆè®­ç»ƒè„šæœ¬ v3
ä¸»è¦æ”¹è¿›ï¼š
1. ä»æœ€ä½³checkpointå¼€å§‹Phase 3
2. åœ¨çº¿æ•°æ®è´¨é‡ç­›é€‰ï¼ˆåªä¿ç•™é«˜è´¨é‡episodeï¼‰
3. Early Stoppingæœºåˆ¶ï¼ˆæ£€æµ‹æ€§èƒ½é€€åŒ–ï¼‰
4. æ•°æ®å¹³è¡¡æ§åˆ¶ï¼ˆé™åˆ¶åœ¨çº¿æ•°æ®æ¯”ä¾‹ï¼‰
5. å­¦ä¹ ç‡è°ƒæ•´ï¼ˆPhase 3ä½¿ç”¨æ›´ä½å­¦ä¹ ç‡ï¼‰
6. æ›´è¯¦ç»†çš„æ€§èƒ½ç›‘æ§
"""
import sys
import os
import pathlib
import hydra
import torch
import numpy as np
import copy
import wandb
import tqdm
import shutil
from omegaconf import OmegaConf
from torch.utils.data import DataLoader, ConcatDataset, Dataset

from diffusion_policy_3d.policy.rl100 import RL100
from diffusion_policy_3d.common.pytorch_util import dict_apply, optimizer_to
from diffusion_policy_3d.model.common.lr_scheduler import get_scheduler
from diffusion_policy_3d.common.checkpoint_util import TopKCheckpointManager
from diffusion_policy_3d.common.replay_buffer import ReplayBuffer
from diffusion_policy_3d.common.sampler import SequenceSampler

from diffusion_policy_3d.env.metaworld.metaworld_wrapper import MetaWorldEnv
from diffusion_policy_3d.gym_util.multistep_wrapper import MultiStepWrapper
import zarr
import os
os.environ["WANDB_MODE"] = "offline"

class MemoryDataset(Dataset):
    """åœ¨çº¿æ•°æ®çš„å†…å­˜æ•°æ®é›†"""
    def __init__(self, data_list, horizon, pad_before, pad_after):
        super().__init__()

        # 1. è§£åŒ…æ•°æ®åˆ—è¡¨
        _data_cache = {
            'point_cloud': [], 'state': [], 'full_state': [],
            'action': [], 'reward': [], 'done': []
        }

        for item in data_list:
            # Point Cloud
            pc = item['obs']['point_cloud']
            if isinstance(pc, torch.Tensor): pc = pc.detach().cpu().numpy()
            if pc.shape[-1] > 3: pc = pc[..., :3]
            _data_cache['point_cloud'].append(pc)

            # Agent Pos (State)
            pos = item['obs']['agent_pos']
            if isinstance(pos, torch.Tensor): pos = pos.detach().cpu().numpy()
            _data_cache['state'].append(pos)

            # Full State
            fs = item['obs'].get('full_state', np.zeros(39, dtype=np.float32))
            if isinstance(fs, torch.Tensor): fs = fs.detach().cpu().numpy()
            _data_cache['full_state'].append(fs)

            # Action
            act = item['action']
            if isinstance(act, torch.Tensor): act = act.detach().cpu().numpy()
            _data_cache['action'].append(act)

            # Reward
            rew = item['reward']
            if isinstance(rew, torch.Tensor): rew = rew.detach().cpu().numpy()
            _data_cache['reward'].append(np.array(rew).reshape(-1)[0])

            # Done
            don = item['done']
            if isinstance(don, torch.Tensor): don = don.detach().cpu().numpy()
            _data_cache['done'].append(np.array(don).reshape(-1)[0])

        # 2. å †å æˆå¤§æ•°ç»„
        buffer_data = {
            'point_cloud': np.stack(_data_cache['point_cloud']),
            'state': np.stack(_data_cache['state']),
            'full_state': np.stack(_data_cache['full_state']),
            'action': np.stack(_data_cache['action']),
            'reward': np.array(_data_cache['reward'], dtype=np.float32),
            'done': np.array(_data_cache['done'], dtype=np.float32)
        }

        # 3. åˆ›å»ºå†…å­˜ Zarr
        store = zarr.MemoryStore()
        root = zarr.group(store=store)
        data_group = root.require_group('data', overwrite=True)
        meta_group = root.require_group('meta', overwrite=True)

        for key, val in buffer_data.items():
            data_group.create_dataset(key, data=val)

        # è®¡ç®— episode_ends
        episode_ends = np.where(buffer_data['done'] > 0.5)[0] + 1
        if len(episode_ends) == 0 or episode_ends[-1] != len(buffer_data['done']):
            episode_ends = np.append(episode_ends, len(buffer_data['done']))
        meta_group.create_dataset('episode_ends', data=episode_ends.astype(np.int64))

        self.replay_buffer = ReplayBuffer(root)

        # 4. åˆå§‹åŒ–åºåˆ—é‡‡æ ·å™¨
        self.sampler = SequenceSampler(
            replay_buffer=self.replay_buffer,
            sequence_length=horizon,
            pad_before=pad_before,
            pad_after=pad_after
        )

    def __len__(self):
        return len(self.sampler)

    def __getitem__(self, idx):
        sample = self.sampler.sample_sequence(idx)

        # æ ¼å¼åŒ–è¾“å‡º
        data = {
            'obs': {
                'point_cloud': sample['point_cloud'].astype(np.float32),
                'agent_pos': sample['state'].astype(np.float32),
                'full_state': sample['full_state'].astype(np.float32),
            },
            'action': sample['action'].astype(np.float32),
            'reward': sample['reward'].astype(np.float32),
            'next_obs': {
                'point_cloud': np.concatenate([sample['point_cloud'][1:], sample['point_cloud'][-1:]], axis=0).astype(np.float32),
                'agent_pos': np.concatenate([sample['state'][1:], sample['state'][-1:]], axis=0).astype(np.float32),
                'full_state': np.concatenate([sample['full_state'][1:], sample['full_state'][-1:]], axis=0).astype(np.float32),
            },
            'done': sample['done'].astype(np.float32)
        }
        return dict_apply(data, torch.from_numpy)


class TrainRL100Improved:
    def __init__(self, cfg: OmegaConf):
        self.cfg = cfg
        self.output_dir = os.getcwd()

        # 1. WandB æ¨¡å¼é…ç½®
        wandb_mode = cfg.logging.get('wandb_mode', 'offline')
        os.environ["WANDB_MODE"] = wandb_mode

        # 2. è®¾ç½®ç§å­
        seed = cfg.training.seed
        torch.manual_seed(seed)
        np.random.seed(seed)

        # 3. è®¾å¤‡
        self.device = torch.device(cfg.training.device)

        # 4. åˆå§‹åŒ–ç¦»çº¿æ•°æ®é›†
        if hasattr(cfg.task.dataset, 'zarr_path') and not os.path.isabs(cfg.task.dataset.zarr_path):
            original_cwd = hydra.utils.get_original_cwd()
            cfg.task.dataset.zarr_path = os.path.join(original_cwd, cfg.task.dataset.zarr_path)
            print(f"[Info] Zarr path corrected to: {cfg.task.dataset.zarr_path}")

        self.offline_dataset = hydra.utils.instantiate(cfg.task.dataset)
        self.normalizer = self.offline_dataset.get_normalizer()

        # ===== æ–°ç­–ç•¥ï¼šåˆ†ç¦»æˆåŠŸæ•°æ®å’Œå…¨éƒ¨æ•°æ® =====
        # æˆåŠŸæ•°æ®é›†ï¼ˆç”¨äºILï¼‰ï¼šåŸå§‹æ¼”ç¤ºæ•°æ® + æˆåŠŸçš„rolloutæ•°æ®
        self.successful_datasets = [self.offline_dataset]  # å‡è®¾æ¼”ç¤ºæ•°æ®éƒ½æ˜¯æˆåŠŸçš„

        # å…¨éƒ¨æ•°æ®é›†ï¼ˆç”¨äºRLï¼‰ï¼šåŸå§‹æ¼”ç¤ºæ•°æ® + æ‰€æœ‰rolloutæ•°æ®
        self.all_rl_datasets = [self.offline_dataset]

        print(f"[Info] æ•°æ®ç®¡ç†ç­–ç•¥: ILä½¿ç”¨æˆåŠŸæ•°æ®ï¼ŒRLä½¿ç”¨å…¨éƒ¨æ•°æ®")

        # 5. åˆå§‹åŒ– RL100 ç­–ç•¥
        self.model: RL100 = hydra.utils.instantiate(cfg.policy)
        self.model.set_normalizer(self.normalizer)
        self.model.to(self.device)

        # 5.5 åˆå§‹åŒ– EMA æ¨¡å‹ï¼ˆç”¨äºç¨³å®šè®­ç»ƒå’Œè¯„ä¼°ï¼‰
        self.ema_model: RL100 = None
        self.ema = None
        if cfg.training.get('use_ema', True):
            try:
                self.ema_model = copy.deepcopy(self.model)
                print("[Info] EMA æ¨¡å‹å·²åˆ›å»ºï¼ˆdeepcopyï¼‰")
            except Exception as e:
                # Minkowski Engine å¯èƒ½æ— æ³• deepcopyï¼Œé‡æ–°å®ä¾‹åŒ–
                print(f"[Warning] EMA deepcopy å¤±è´¥: {e}ï¼Œå°è¯•é‡æ–°å®ä¾‹åŒ–...")
                self.ema_model = hydra.utils.instantiate(cfg.policy)
                self.ema_model.set_normalizer(self.normalizer)
                self.ema_model.load_state_dict(self.model.state_dict())
                print("[Info] EMA æ¨¡å‹å·²é‡æ–°å®ä¾‹åŒ–")

            self.ema_model.to(self.device)

            # å¯¼å…¥ EMAModel
            from diffusion_policy_3d.model.diffusion.ema_model import EMAModel
            self.ema = EMAModel(
                model=self.ema_model,
                power=cfg.training.get('ema_power', 0.999)
            )
            print(f"[Info] EMA å·²å¯ç”¨ (power={cfg.training.get('ema_power', 0.999)})")
        else:
            print("[Info] EMA å·²ç¦ç”¨")

        # 6. ä¼˜åŒ–å™¨ï¼ˆç»Ÿä¸€ä½¿ç”¨å•ä¸€ä¼˜åŒ–å™¨ï¼Œé¿å…å­¦ä¹ ç‡è°ƒåº¦é—®é¢˜ï¼‰
        # ã€ä¿®å¤ã€‘åˆ é™¤å¤šä½™çš„ä¼˜åŒ–å™¨ï¼Œç»Ÿä¸€ä½¿ç”¨self.optimizer
        # ä¸åŒé˜¶æ®µé€šè¿‡å†»ç»“/è§£å†»å‚æ•°æ¥æ§åˆ¶æ›´æ–°èŒƒå›´
        self.optimizer = hydra.utils.instantiate(
            cfg.optimizer, params=self.model.parameters())

        # 7. å­¦ä¹ ç‡è°ƒåº¦å™¨
        self.lr_scheduler = None
        if hasattr(cfg, 'lr_scheduler') and cfg.lr_scheduler is not None:
            self.lr_scheduler = get_scheduler(
                cfg.lr_scheduler.name,
                optimizer=self.optimizer,
                num_warmup_steps=cfg.lr_scheduler.warmup_steps,
                num_training_steps=cfg.training.total_steps
            )

        # 8. TopK Checkpointç®¡ç†å™¨
        self.ckpt_manager = TopKCheckpointManager(
            save_dir=os.path.join(self.output_dir, 'checkpoints2'),
            monitor_key='eval_success_rate',
            mode='max',
            k=cfg.training.get('keep_top_k_checkpoints', 3),
            format_str='epoch_{epoch:04d}_success_{eval_success_rate:.3f}.ckpt'
        )

        # 9. ç¯å¢ƒï¼ˆç”¨äºrolloutï¼‰ - ä½¿ç”¨MultiStepWrapperä¸è¯„ä¼°ä¿æŒä¸€è‡´
        num_points = cfg.task.dataset.pointcloud_encoder_cfg.in_channels_points \
                     if 'in_channels_points' in cfg.task.dataset.pointcloud_encoder_cfg else 1024

        # ç¨€ç–å¥–åŠ±é…ç½®
        use_sparse_reward = cfg.training.get('use_sparse_reward', False)
        sparse_reward_value = cfg.training.get('sparse_reward_value', 1.0)

        # åˆ›å»ºåŸå§‹ç¯å¢ƒ
        raw_env = MetaWorldEnv(
            task_name=cfg.task.env_name,
            device=cfg.training.device,
            num_points=num_points,
            use_sparse_reward=use_sparse_reward,
            sparse_reward_value=sparse_reward_value
        )

        # åŒ…è£…ä¸ºMultiStepWrapperï¼ˆä¸è¯„ä¼°ä¿æŒä¸€è‡´ï¼šn_obs_steps=2, n_action_steps=8ï¼‰
        self.env = MultiStepWrapper(
            raw_env,
            n_obs_steps=cfg.policy.n_obs_steps,  # 2
            n_action_steps=cfg.policy.n_action_steps,  # 8
            max_episode_steps=200,
            reward_agg_method='sum',
        )
        print(f"[Info] Rolloutç¯å¢ƒå·²ä½¿ç”¨MultiStepWrapperï¼ˆn_obs_steps={cfg.policy.n_obs_steps}, n_action_steps={cfg.policy.n_action_steps}ï¼‰")

        # 10. è¯„ä¼°ç¯å¢ƒï¼ˆå‘¨æœŸæ€§éªŒè¯ï¼‰
        self.eval_runner = None
        if hasattr(cfg.task, 'env_runner'):
            self.eval_runner = hydra.utils.instantiate(cfg.task.env_runner,output_dir=os.getcwd())

        self.global_step = 0
        self.best_success_rate = 0.0

        # ===== æ€§èƒ½è¿½è¸ª =====
        self.best_checkpoint_path = None

        # 11. åœ¨çº¿RLçš„Replay Buffer
        self.online_replay_buffer = []
        self.online_replay_max_size = cfg.training.get('online_replay_size', 5000)

        # 12. WandB åˆå§‹åŒ–
        wandb.init(
            project=cfg.logging.get('project', "RL100-Improved-v3"),
            name=cfg.logging.name,
            config=OmegaConf.to_container(cfg, resolve=True),
            mode=wandb_mode
        )

        # 13. æ‰“å°é…ç½®ä¿¡æ¯
        self._print_config()

    def _print_config(self):
        cfg = self.cfg
        print("\n" + "="*70)
        print("RL100 æ”¹è¿›ç‰ˆv3 é…ç½®")
        print("="*70)
        print(f"ğŸ“Š æ•°æ®é›†: {cfg.task.name}")
        print(f"ğŸ¯ ä»»åŠ¡: {cfg.task.env_name}")
        print(f"ğŸ’¾ æ•°æ®è·¯å¾„: {cfg.task.dataset.zarr_path}")
        print(f"ğŸ”§ è®¾å¤‡: {cfg.training.device}")
        print(f"ğŸŒ± éšæœºç§å­: {cfg.training.seed}")
        print("\nâœ¨ æ–°å¢æ”¹è¿›:")
        print(f"  âœ“ æ•°æ®è´¨é‡ç­›é€‰ï¼ˆæˆåŠŸç‡é˜ˆå€¼: {cfg.training.get('data_quality_threshold', 0.5)}ï¼‰")
        print(f"  âœ“ Phase 3ä»æœ€ä½³checkpointå¼€å§‹")
        print(f"  âœ“ å­¦ä¹ ç‡è¡°å‡ï¼ˆPhase 3: {cfg.training.get('phase3_lr_factor', 0.1)}xï¼‰")

        # å¥–åŠ±æ¨¡å¼ä¿¡æ¯
        if cfg.training.get('use_sparse_reward', False):
            print(f"\nâš ï¸  å¥–åŠ±æ¨¡å¼: ç¨€ç–å¥–åŠ±ï¼ˆæˆåŠŸ={cfg.training.get('sparse_reward_value', 1.0)}, å¤±è´¥=0ï¼‰")
        else:
            print(f"\nğŸ’° å¥–åŠ±æ¨¡å¼: å¯†é›†å¥–åŠ±ï¼ˆè·ç¦»+æˆåŠŸbonusï¼‰")

        print("\næ ¸å¿ƒç‰¹æ€§:")
        print("  âœ“ æ‰©æ•£æ¨¡å‹é¢„æµ‹å™ªå£°ï¼ˆepsilonï¼‰")
        print("  âœ“ åŒå±‚MDPï¼ˆç¯å¢ƒçº§ + å»å™ªæ­¥çº§ï¼‰")
        print(f"  âœ“ IQLç®—æ³•ï¼ˆexpectile={cfg.policy.omega}ï¼‰")
        print(f"  âœ“ AM-Qç­›é€‰ï¼ˆthreshold={cfg.policy.amq_threshold}ï¼‰")
        print(f"  âœ“ VIBæ­£åˆ™åŒ–ï¼ˆIL: recon=1.0, kl=0.001 | RL: recon={cfg.policy.beta_recon_rl}, kl={cfg.policy.beta_kl_rl}ï¼‰")
        print(f"  âœ“ GAEä¼˜åŠ¿ä¼°è®¡ï¼ˆlambda={cfg.policy.gae_lambda}ï¼‰")
        print("\nè®­ç»ƒæµç¨‹:")
        print(f"  Phase 1: ILé¢„è®­ç»ƒ - {cfg.training.il_epochs} epochs")
        print(f"  Phase 2: ç¦»çº¿RL - {cfg.training.rl_iterations}è½®è¿­ä»£ Ã— {cfg.training.rl_epochs_per_iter} epochs")
        if cfg.training.get('online_rl_enabled', True):
            print(f"  Phase 3: åœ¨çº¿RL - {cfg.training.get('online_rl_epochs', 10)} epochs")
        print("="*70 + "\n")

    def run(self):
        cfg = self.cfg

        # =========================================
        # Phase 1: Imitation Learning
        # =========================================
        resume_path = cfg.training.get('resume_path', None)
        #resume_path = "/nfs_global/S/yangrongzheng/3D-Diffusion-Policy/3D-Diffusion-Policy/checkpoints2/checkpoint_il_final.ckpt"
        if resume_path is not None and os.path.exists(resume_path):
            print(f"\n[Info] ä»Checkpointæ¢å¤: {resume_path}")
            self.load_checkpoint(resume_path)
            # ä»checkpointæ¢å¤åä¹Ÿéœ€è¦å†»ç»“encoder
            print("\n[Info] Freezing encoder after resume...")
            self.freeze_encoder()
        else:
            print("\n" + "="*70)
            print("Phase 1: æ¨¡ä»¿å­¦ä¹ é¢„è®­ç»ƒï¼ˆILï¼‰")
            print("="*70)
            self.model.switch_to_il_mode()

            il_epochs = cfg.training.il_epochs
            self.train_loop(
                epochs=il_epochs,
                mode='il',
                desc="IL Pre-training"
            )
            self.save_checkpoint("checkpoint_il_final.ckpt")
            print("âœ“ Phase 1 å®Œæˆ\n")

            # ILå®Œæˆåï¼Œå†»ç»“encoder
            print("\n[Info] Freezing encoder after IL phase...")
            self.freeze_encoder()

        # =========================================
        # Phase 2: ç¦»çº¿RLè¿­ä»£
        # =========================================
        print("="*70)
        print("Phase 2: ç¦»çº¿å¼ºåŒ–å­¦ä¹ è¿­ä»£ï¼ˆIQL + AM-Qï¼‰")
        print("="*70)

        M = cfg.training.rl_iterations
        eval_freq = cfg.training.get('eval_freq', 2)
        data_quality_threshold = cfg.training.get('data_quality_threshold', 0.5)

        for iteration in range(M):
            print(f"\n{'â”€'*70}")
            print(f"è¿­ä»£ {iteration+1}/{M}")
            print(f"{'â”€'*70}")

            # ===== ã€æ–°å¢ã€‘Step 1: IQLé¢„è®­ç»ƒQã€Vç½‘ç»œ =====
            print(f"[1/6] IQLé¢„è®­ç»ƒQ/Vç½‘ç»œ...")
            self.model.switch_to_rl_mode()

            iql_epochs = self.cfg.training.get('iql_epochs_per_iter', 10)  # é»˜è®¤10ä¸ªepoch
            self.train_loop(
                epochs=iql_epochs,
                mode='iql',
                desc=f"IQL Pre-train Iter {iteration+1}"
            )

            # ===== ã€æ–°å¢ã€‘Step 2: ç¡¬æ›´æ–°targetç½‘ç»œ =====
            print(f"\n[2/6] Hard update target Q networks...")
            self.model.target_q_net1.load_state_dict(self.model.q_net1.state_dict())
            if self.model.is_double_q:
                self.model.target_q_net2.load_state_dict(self.model.q_net2.state_dict())
            if self.model.use_hierarchical_mdp:
                self.model.target_diffusion_q_net1.load_state_dict(self.model.diffusion_q_net1.state_dict())
                if self.model.is_double_q:
                    self.model.target_diffusion_q_net2.load_state_dict(self.model.diffusion_q_net2.state_dict())

            # Step 3: ç¦»çº¿RLè®­ç»ƒï¼ˆä¼˜åŒ–ç­–ç•¥ï¼‰
            print(f"\n[3/6] ç¦»çº¿RLè®­ç»ƒï¼ˆAM-Qä¼˜åŠ¿åŠ æƒï¼‰...")

            rl_epochs_per_iter = self.cfg.training.rl_epochs_per_iter
            self.train_loop(
                epochs=rl_epochs_per_iter,
                mode='offline_rl',
                desc=f"Offline RL Iter {iteration+1}"
            )

            # Step 4: Rolloutæ”¶é›†æ–°æ•°æ®
            print(f"\n[4/6] Rolloutæ”¶é›†æ–°æ•°æ®...")
            all_data, success_data, rollout_stats = self.collect_rollout_data(
                num_episodes=cfg.training.rollout_episodes
            )

            # è®°å½•rolloutç»Ÿè®¡
            wandb.log({
                'rollout/mean_reward': rollout_stats['mean_reward'],
                'rollout/mean_length': rollout_stats['mean_length'],
                'rollout/success_rate': rollout_stats['success_rate']
            }, step=self.global_step)

            # Step 3: æ•°æ®è´¨é‡æ£€æµ‹ä¸åˆå¹¶
            data_quality_threshold = cfg.training.get('data_quality_threshold', 0.3)

            if len(all_data) > 0:
                print(f"\n[5/6] æ•°æ®è´¨é‡æ£€æµ‹ä¸åˆå¹¶...")

                # ===== æ•°æ®è´¨é‡æ£€æµ‹ï¼ˆä¸åšæ•°é‡é™åˆ¶ï¼Œåªæ£€æµ‹è´¨é‡ï¼‰=====
                if rollout_stats['success_rate'] >= data_quality_threshold:
                    print(f"  âœ“ æ•°æ®è´¨é‡åˆæ ¼ï¼ˆ{rollout_stats['success_rate']:.1%} >= {data_quality_threshold:.1%}ï¼‰")

                    # åˆ›å»ºå…¨éƒ¨æ•°æ®çš„æ•°æ®é›†ï¼ˆç”¨äºRLï¼‰
                    all_dataset = MemoryDataset(
                        data_list=all_data,
                        horizon=cfg.task.dataset.horizon,
                        pad_before=cfg.task.dataset.pad_before,
                        pad_after=cfg.task.dataset.pad_after
                    )
                    self.all_rl_datasets.append(all_dataset)
                    print(f"  ğŸ“¦ RLæ•°æ®é›†: +{len(all_data)}æ­¥ â†’ æ€»è®¡{len(self.all_rl_datasets)}ä¸ªæ•°æ®é›†")

                    # å¦‚æœæœ‰æˆåŠŸçš„episodeï¼Œåˆ›å»ºæˆåŠŸæ•°æ®é›†ï¼ˆç”¨äºILï¼‰
                    if len(success_data) > 0:
                        success_dataset = MemoryDataset(
                            data_list=success_data,
                            horizon=cfg.task.dataset.horizon,
                            pad_before=cfg.task.dataset.pad_before,
                            pad_after=cfg.task.dataset.pad_after
                        )
                        self.successful_datasets.append(success_dataset)
                        print(f"  âœ… ILæ•°æ®é›†ï¼ˆæˆåŠŸï¼‰: +{len(success_data)}æ­¥ â†’ æ€»è®¡{len(self.successful_datasets)}ä¸ªæ•°æ®é›†")
                    else:
                        print(f"  âš ï¸  æœ¬è½®æ— æˆåŠŸepisodeï¼ŒILæ•°æ®é›†ä¸æ›´æ–°")

                else:
                    print(f"  âœ— æ•°æ®è´¨é‡ä¸åˆæ ¼ï¼ˆ{rollout_stats['success_rate']:.1%} < {data_quality_threshold:.1%}ï¼‰ï¼Œä¸¢å¼ƒæœ¬è½®æ•°æ®")
            else:
                print(f"\n[5/6] æ— æ–°æ•°æ®æ”¶é›†ï¼Œè·³è¿‡åˆå¹¶")

            # Step 4: ILè‡ªæˆ‘çº æ­£ï¼ˆå¯é€‰ï¼Œencoderä¿æŒå†»ç»“ï¼‰
            il_finetune_enabled = cfg.training.get('il_finetune_enabled', True)
            if il_finetune_enabled:
                print(f"\n[6/6] ILå¾®è°ƒï¼ˆåªä½¿ç”¨æˆåŠŸæ•°æ®ï¼Œencoderå†»ç»“ï¼‰...")
                self.model.switch_to_il_mode()
                self.train_loop(
                    epochs=cfg.training.il_finetune_epochs,
                    mode='il',
                    desc=f"IL Finetune Iter {iteration+1}"
                )
            else:
                print(f"\n[6/6] ILå¾®è°ƒå·²ç¦ç”¨ï¼ˆè·³è¿‡ï¼‰")

            # Step 5: å‘¨æœŸæ€§è¯„ä¼°
            if (iteration + 1) % eval_freq == 0 or iteration == M - 1:
                print(f"\n[è¯„ä¼°] éªŒè¯ç­–ç•¥æ€§èƒ½...")
                eval_results = self.evaluate_policy()

                # ä¿å­˜æœ€ä½³æ¨¡å‹
                success_rate = eval_results['success_rate']

                # ===== æ›´æ–°best_success_rateå¹¶ä¿å­˜checkpoint =====
                if success_rate > self.best_success_rate:
                    self.best_success_rate = success_rate
                    print(f"  ğŸ† æ–°æœ€ä½³æˆåŠŸç‡: {success_rate:.2%}")
                else:
                    print(f"  ğŸ“Š å½“å‰æˆåŠŸç‡: {success_rate:.2%}")

                # ä¿å­˜TopK checkpoint
                ckpt_path = self.ckpt_manager.get_ckpt_path({
                    'eval_success_rate': success_rate,
                    'epoch': iteration + 1
                })

                if ckpt_path is not None:
                    checkpoint = {
                        'model_state_dict': self.model.state_dict(),
                        'optimizer_state_dict': self.optimizer.state_dict(),
                        'normalizer_state_dict': self.normalizer.state_dict(),
                        'global_step': self.global_step,
                        'best_success_rate': self.best_success_rate,  # ç°åœ¨æ˜¯æ­£ç¡®çš„æœ€æ–°å€¼
                        'current_success_rate': success_rate,  # æ·»åŠ å½“å‰æˆåŠŸç‡å­—æ®µ
                        'config': self.cfg,
                    }
                    if self.lr_scheduler is not None:
                        checkpoint['lr_scheduler_state_dict'] = self.lr_scheduler.state_dict()

                    torch.save(checkpoint, ckpt_path)
                    print(f"ğŸ’¾ ä¿å­˜TopKæ¨¡å‹: {os.path.basename(ckpt_path)}")

                    # å¦‚æœæ˜¯æ–°æœ€ä½³æˆåŠŸç‡ï¼Œè®°å½•checkpointè·¯å¾„
                    if success_rate == self.best_success_rate:
                        self.best_checkpoint_path = ckpt_path

            # ä¿å­˜å½“å‰è¿­ä»£checkpoint
            self.save_checkpoint(f"checkpoint_iter_{iteration+1}.ckpt")

        print("\nâœ“ Phase 2 å®Œæˆ")
        print(f"æœ€ä½³æˆåŠŸç‡: {self.best_success_rate:.2%}")
        print(f"æœ€ä½³Checkpoint: {self.best_checkpoint_path}")

        # =========================================
        # Phase 3: åœ¨çº¿RLï¼ˆä»æœ€ä½³checkpointå¼€å§‹ï¼‰
        # =========================================
        if cfg.training.get('online_rl_enabled', True):
            print("\n" + "="*70)
            print("Phase 3: åœ¨çº¿å¼ºåŒ–å­¦ä¹ ï¼ˆä»æœ€ä½³checkpointå¼€å§‹ï¼‰")
            print("="*70)

            # ===== æ”¹è¿›ï¼šåŠ è½½æœ€ä½³checkpoint =====
            if self.best_checkpoint_path is not None and os.path.exists(self.best_checkpoint_path):
                print(f"ğŸ“‚ åŠ è½½æœ€ä½³checkpoint: {os.path.basename(self.best_checkpoint_path)}")
                self.load_checkpoint(self.best_checkpoint_path)
            else:
                print("âš ï¸  æœªæ‰¾åˆ°æœ€ä½³checkpointï¼Œä»å½“å‰çŠ¶æ€ç»§ç»­")

            self.model.switch_to_rl_mode()

            # ===== æ”¹è¿›ï¼šé™ä½å­¦ä¹ ç‡ =====
            phase3_lr_factor = cfg.training.get('phase3_lr_factor', 0.1)
            original_lr = self.optimizer.param_groups[0]['lr']
            for param_group in self.optimizer.param_groups:
                param_group['lr'] = original_lr * phase3_lr_factor
            print(f"ğŸ”½ é™ä½å­¦ä¹ ç‡: {original_lr:.2e} -> {original_lr * phase3_lr_factor:.2e}")

            online_epochs = cfg.training.get('online_rl_epochs', 10)
            for epoch in range(online_epochs):
                print(f"\nåœ¨çº¿RL Epoch {epoch+1}/{online_epochs}")

                # æ”¶é›†åœ¨çº¿æ•°æ®
                all_online_data, success_online_data, rollout_stats = self.collect_rollout_data(
                    num_episodes=cfg.training.get('online_rollout_episodes', 10)
                )

                wandb.log({
                    'online_rl/mean_reward': rollout_stats['mean_reward'],
                    'online_rl/success_rate': rollout_stats['success_rate']
                }, step=self.global_step)

                if len(all_online_data) > 0:
                    # æ·»åŠ åˆ°replay bufferï¼ˆåœ¨çº¿RLä½¿ç”¨å…¨éƒ¨æ•°æ®ï¼‰
                    self.online_replay_buffer.extend(all_online_data)

                    # é™åˆ¶bufferå¤§å°
                    if len(self.online_replay_buffer) > self.online_replay_max_size:
                        # ç§»é™¤æœ€æ—©çš„æ•°æ®
                        self.online_replay_buffer = self.online_replay_buffer[-self.online_replay_max_size:]
                        print(f"  ğŸ“¦ Replay Bufferå·²æ»¡ï¼Œä¿ç•™æœ€è¿‘{self.online_replay_max_size}æ¡æ•°æ®")

                    # ä½¿ç”¨replay bufferä¸­çš„æ‰€æœ‰æ•°æ®è®­ç»ƒ
                    print(f"  ğŸ“Š ä½¿ç”¨Replay Bufferè®­ç»ƒï¼ˆå¤§å°: {len(self.online_replay_buffer)}ï¼‰")
                    online_dataset = MemoryDataset(
                        data_list=self.online_replay_buffer,
                        horizon=cfg.task.dataset.horizon,
                        pad_before=cfg.task.dataset.pad_before,
                        pad_after=cfg.task.dataset.pad_after
                    )

                    # è®­ç»ƒ
                    online_dataloader = DataLoader(
                        online_dataset,
                        batch_size=cfg.dataloader.batch_size,
                        shuffle=True,
                        num_workers=cfg.dataloader.num_workers,
                        pin_memory=True
                    )

                    # è®­ç»ƒå¤šè½®ä»¥å……åˆ†åˆ©ç”¨replay buffer
                    train_epochs = 2 if epoch < online_epochs // 2 else 1
                    self.train_loop_online(
                        dataloader=online_dataloader,
                        epochs=train_epochs,
                        desc=f"Online RL Epoch {epoch+1}"
                    )

            # æœ€ç»ˆè¯„ä¼°
            print(f"\n[æœ€ç»ˆè¯„ä¼°]")
            final_eval = self.evaluate_policy()
            self.save_checkpoint("checkpoint_online_rl_final.ckpt")
            print(f"âœ“ Phase 3 å®Œæˆ - æœ€ç»ˆæˆåŠŸç‡: {final_eval['success_rate']:.2%}\n")
        else:
            print("\nâœ— Phase 3 è·³è¿‡ï¼ˆonline_rl_enabled=falseï¼‰\n")

        # =========================================
        # è®­ç»ƒå®Œæˆæ€»ç»“
        # =========================================
        print("="*70)
        print("è®­ç»ƒå®Œæˆæ€»ç»“")
        print("="*70)
        print(f"âœ“ Phase 1: ILé¢„è®­ç»ƒ - {cfg.training.il_epochs} epochs")
        print(f"âœ“ Phase 2: ç¦»çº¿RL - æœ€ä½³æˆåŠŸç‡ {self.best_success_rate:.2%}")
        if cfg.training.get('online_rl_enabled', True):
            print(f"âœ“ Phase 3: åœ¨çº¿RL - æœ€ç»ˆæˆåŠŸç‡ {final_eval['success_rate']:.2%}")
        print(f"\næ€»è®­ç»ƒæ­¥æ•°: {self.global_step:,}")
        print(f"æœ€ä½³Checkpoint: {self.best_checkpoint_path}")
        print("="*70)

    def get_dataloader(self, mode='rl'):
        """
        åŠ¨æ€æ„å»ºDataLoader

        Args:
            mode: 'il' - ä½¿ç”¨æˆåŠŸæ•°æ®ï¼ˆILè®­ç»ƒï¼‰ï¼Œ'rl' - ä½¿ç”¨å…¨éƒ¨æ•°æ®ï¼ˆRLè®­ç»ƒï¼‰
        """
        if mode == 'il':
            # ILæ¨¡å¼ï¼šåªä½¿ç”¨æˆåŠŸçš„æ•°æ®
            combined_dataset = ConcatDataset(self.successful_datasets)
            print(f"  [Dataloader] ILæ¨¡å¼ - ä½¿ç”¨{len(self.successful_datasets)}ä¸ªæˆåŠŸæ•°æ®é›†")
        else:
            # RLæ¨¡å¼ï¼šä½¿ç”¨æ‰€æœ‰æ•°æ®
            combined_dataset = ConcatDataset(self.all_rl_datasets)
            print(f"  [Dataloader] RLæ¨¡å¼ - ä½¿ç”¨{len(self.all_rl_datasets)}ä¸ªæ•°æ®é›†ï¼ˆåŒ…æ‹¬å…¨éƒ¨rolloutï¼‰")

        return DataLoader(
            combined_dataset,
            batch_size=self.cfg.dataloader.batch_size,
            shuffle=True,
            num_workers=self.cfg.dataloader.num_workers,
            pin_memory=True
        )

    def train_loop(self, epochs, mode, desc):
        """é€šç”¨è®­ç»ƒå¾ªç¯ï¼ˆIL + IQL + ç¦»çº¿RLï¼‰"""
        # ã€ä¿®å¤ã€‘ç»Ÿä¸€ä½¿ç”¨self.optimizerï¼Œé¿å…å­¦ä¹ ç‡è°ƒåº¦å™¨ä¸åŒ¹é…é—®é¢˜
        # æ ¹æ®æ¨¡å¼é€‰æ‹©å¯¹åº”çš„æ•°æ®é›†
        if mode == 'il':
            dataloader = self.get_dataloader(mode='il')
        elif mode == 'iql':
            dataloader = self.get_dataloader(mode='rl')
        elif mode == 'offline_rl':
            dataloader = self.get_dataloader(mode='rl')
        else:
            raise ValueError(f"Unknown mode: {mode}")

        # ç»Ÿä¸€ä½¿ç”¨self.optimizer
        optimizer = self.optimizer

        target_update_freq = self.cfg.training.get('target_update_freq', 30)

        for epoch in range(epochs):
            with tqdm.tqdm(dataloader, desc=f"{desc} Epoch {epoch+1}", leave=False) as tepoch:
                for batch in tepoch:
                    # 1. æ•°æ®æ¬è¿
                    batch = dict_apply(batch, lambda x: x.to(self.device, non_blocking=True))

                    # 2. æ ¹æ®æ¨¡å¼è®¡ç®—Loss
                    if mode == 'il':
                        raw_loss, loss_dict = self.model.compute_loss(batch)
                    elif mode == 'iql':
                        # IQLæ¨¡å¼ï¼šåªæ›´æ–°Qã€Vç½‘ç»œ
                        raw_loss, loss_dict = self.model.compute_iql_loss(batch)
                    elif mode == 'offline_rl':
                        raw_loss, loss_dict = self.model.compute_offline_rl_loss(batch)

                    # 3. åå‘ä¼ æ’­
                    optimizer.zero_grad()
                    raw_loss.backward()

                    # æ¢¯åº¦è£å‰ª
                    grad_norm = torch.nn.utils.clip_grad_norm_(
                        self.model.parameters(),
                        self.cfg.training.get('grad_clip_norm', 10.0)
                    )

                    optimizer.step()

                    # å­¦ä¹ ç‡è°ƒåº¦ï¼ˆæ‰€æœ‰é˜¶æ®µç»Ÿä¸€è°ƒåº¦ï¼‰
                    if self.lr_scheduler is not None:
                        self.lr_scheduler.step()

                    # ===== EMA æ›´æ–°ï¼ˆæ‰€æœ‰é˜¶æ®µç»Ÿä¸€æ›´æ–°ï¼‰=====
                    if self.ema is not None:
                        self.ema.step(self.model)

                    # 4. ä¼˜åŒ–çš„Target Qç½‘ç»œæ›´æ–°
                    if mode in ['iql', 'offline_rl'] and self.global_step % target_update_freq == 0:
                        self.model.update_target_q_networks(tau=0.005)

                    # 5. è¯¦ç»†æ—¥å¿—
                    self.global_step += 1
                    loss_dict['train/grad_norm'] = grad_norm.item()
                    if self.lr_scheduler is not None:
                        loss_dict['train/lr'] = self.lr_scheduler.get_last_lr()[0]

                    wandb.log(loss_dict, step=self.global_step)

                    # 6. æ›´è¯¦ç»†çš„è¿›åº¦æ¡æ˜¾ç¤º
                    if mode == 'iql':
                        # IQLæ¨¡å¼ï¼šæ˜¾ç¤ºQã€V loss
                        postfix_dict = {
                            'total': f"{raw_loss.item():.4f}",
                            'q': f"{loss_dict.get('iql/q', 0):.4f}",
                            'v': f"{loss_dict.get('iql/v', 0):.4f}",
                            'grad': f"{grad_norm.item():.3f}"
                        }
                    elif mode == 'offline_rl':
                        # ç¦»çº¿RLæ¨¡å¼ï¼šæ˜¾ç¤ºdiffusion, q, vä¸‰ä¸ªä¸»è¦loss
                        postfix_dict = {
                            'total': f"{raw_loss.item():.4f}",
                            'diff': f"{loss_dict.get('loss/diffusion', 0):.4f}",
                            'q': f"{loss_dict.get('loss/q', 0):.4f}",
                            'v': f"{loss_dict.get('loss/v', 0):.4f}",
                            'grad': f"{grad_norm.item():.3f}"
                        }
                    else:
                        # ILæ¨¡å¼ï¼šç®€å•æ˜¾ç¤º
                        postfix_dict = {
                            'loss': f"{raw_loss.item():.4f}",
                            'grad': f"{grad_norm.item():.3f}"
                        }
                    tepoch.set_postfix(postfix_dict)

    def train_loop_online(self, dataloader, epochs, desc):
        """åœ¨çº¿RLè®­ç»ƒå¾ªç¯ï¼ˆä½¿ç”¨GAE + Q/Vç½‘ç»œæ›´æ–°ï¼‰"""
        target_update_freq = self.cfg.training.get('target_update_freq', 30)

        for epoch in range(epochs):
            with tqdm.tqdm(dataloader, desc=f"{desc} (GAE)", leave=False) as tepoch:
                for batch in tepoch:
                    batch = dict_apply(batch, lambda x: x.to(self.device, non_blocking=True))

                    # ä½¿ç”¨GAEè®¡ç®—ä¼˜åŠ¿ï¼ˆç°åœ¨åŒ…å«Q/Vç½‘ç»œæ›´æ–°ï¼‰
                    raw_loss, loss_dict = self.model.compute_online_rl_loss(batch)

                    self.optimizer.zero_grad()
                    raw_loss.backward()

                    grad_norm = torch.nn.utils.clip_grad_norm_(
                        self.model.parameters(),
                        self.cfg.training.get('grad_clip_norm', 10.0)
                    )

                    self.optimizer.step()

                    if self.lr_scheduler is not None:
                        self.lr_scheduler.step()

                    # ===== EMA æ›´æ–° =====
                    if self.ema is not None:
                        self.ema.step(self.model)

                    # ===== æ·»åŠ Target Qç½‘ç»œæ›´æ–° =====
                    if self.global_step % target_update_freq == 0:
                        self.model.update_target_q_networks(tau=0.005)

                    # æ—¥å¿—
                    self.global_step += 1
                    loss_dict['online_rl/grad_norm'] = grad_norm.item()
                    if self.lr_scheduler is not None:
                        loss_dict['online_rl/lr'] = self.lr_scheduler.get_last_lr()[0]

                    wandb.log(loss_dict, step=self.global_step)

                    # è¯¦ç»†çš„è¿›åº¦æ¡æ˜¾ç¤ºï¼ˆåœ¨çº¿RLä¹Ÿæ˜¾ç¤ºå„ä¸ªlossç»„ä»¶ï¼‰
                    postfix_dict = {
                        'total': f"{raw_loss.item():.4f}",
                        'diff': f"{loss_dict.get('loss/diffusion', 0):.4f}",
                        'q': f"{loss_dict.get('loss/q', 0):.4f}",
                        'v': f"{loss_dict.get('loss/v', 0):.4f}",
                        'grad': f"{grad_norm.item():.3f}"
                    }
                    tepoch.set_postfix(postfix_dict)

    def collect_rollout_data(self, num_episodes):
        """
        æ”¶é›†rolloutæ•°æ®å¹¶è¿”å›ç»Ÿè®¡ä¿¡æ¯ï¼ˆä½¿ç”¨MultiStepWrapperï¼‰

        Returns:
            all_data: æ‰€æœ‰æ­¥çš„æ•°æ®ï¼ˆç”¨äºRLï¼‰
            success_data: åªåŒ…å«æˆåŠŸepisodeçš„æ•°æ®ï¼ˆç”¨äºILï¼‰
            stats: ç»Ÿè®¡ä¿¡æ¯
        """
        all_data = []
        success_data = []

        # ===== ä½¿ç”¨ EMA æ¨¡å‹è¿›è¡Œè¯„ä¼°ï¼ˆå¦‚æœå¯ç”¨ï¼‰=====
        policy = self.ema_model if self.ema_model is not None else self.model
        policy.eval()

        n_action_steps = self.cfg.policy.n_action_steps  # 3 (æ‰§è¡Œ3æ­¥)
        episode_rewards = []
        episode_lengths = []
        episode_successes = []

        with torch.no_grad():
            for i in range(num_episodes):
                # MultiStepWrapperçš„reset()è¿”å›stacked obs (n_obs_steps, ...)
                obs_dict = self.env.reset()
                done = False

                steps = 0
                episode_reward = 0
                episode_data = []  # å­˜å‚¨å½“å‰episodeçš„æ‰€æœ‰æ•°æ®

                while not done:
                    # 1. å‡†å¤‡è¾“å…¥ï¼ˆobså·²ç»æ˜¯stackedæ ¼å¼ï¼‰
                    input_obs = {}
                    input_obs['point_cloud'] = torch.from_numpy(obs_dict['point_cloud']).unsqueeze(0).to(self.device).float()
                    input_obs['agent_pos'] = torch.from_numpy(obs_dict['agent_pos']).unsqueeze(0).to(self.device).float()

                    # 2. ç­–ç•¥é¢„æµ‹ï¼ˆä½¿ç”¨ EMA æ¨¡å‹ï¼Œé¢„æµ‹horizon=4æ­¥ï¼‰
                    result = policy.predict_action(input_obs)
                    action_full = result['action'].squeeze(0).cpu().numpy()  # (horizon=4, action_dim)

                    # 3. åªå–å‰n_action_stepsæ­¥æ‰§è¡Œï¼ˆ3æ­¥ï¼‰
                    action_to_execute = action_full[:n_action_steps]  # (3, action_dim)

                    # 4. ç¯å¢ƒæ‰§è¡Œï¼ˆMultiStepWrapperä¼šæ‰§è¡Œè¿™3æ­¥ï¼‰
                    next_obs_dict, reward, done, info = self.env.step(action_to_execute)
                    episode_reward += reward

                    # 5. å¤„ç†MultiStepWrapperè¿”å›çš„æ ¼å¼
                    done = np.all(done) if isinstance(done, np.ndarray) else done

                    # MetaWorldçš„æˆåŠŸåˆ¤æ–­ï¼ˆMultiStepWrapperè¿”å›çš„info['success']å¯èƒ½æ˜¯listï¼‰
                    if isinstance(info['success'], (list, np.ndarray)):
                        current_success = max(info['success'])
                    else:
                        current_success = info['success']

                    # 6. ç»„è£…æ ·æœ¬ï¼ˆæ³¨æ„ï¼šè¿™é‡Œå­˜å‚¨çš„æ˜¯å•æ­¥çš„æ•°æ®ï¼Œç”¨äºåç»­è®­ç»ƒï¼‰
                    # ç”±äºMultiStepWrapperæ‰§è¡Œäº†3æ­¥ï¼Œæˆ‘ä»¬ç®€åŒ–ä¸ºå­˜å‚¨ç¬¬ä¸€æ­¥çš„obså’Œå®Œæ•´çš„actionåºåˆ—
                    sample = {
                        'obs': dict_apply(obs_dict, lambda x: torch.from_numpy(x.copy()).float()),
                        'action': torch.from_numpy(action_to_execute[0]).float(),  # å­˜å‚¨æ‰§è¡Œçš„ç¬¬ä¸€æ­¥
                        'reward': torch.tensor(reward).float(),  # MultiStepWrapperå·²ç»èšåˆäº†3æ­¥çš„å¥–åŠ±
                        'next_obs': dict_apply(next_obs_dict, lambda x: torch.from_numpy(x.copy()).float()),
                        'done': torch.tensor(float(done)).float()
                    }
                    episode_data.append(sample)

                    # 7. æ›´æ–°çŠ¶æ€
                    obs_dict = next_obs_dict
                    steps += n_action_steps  # æ‰§è¡Œäº†3æ­¥

                # è®°å½•episodeç»Ÿè®¡
                episode_rewards.append(episode_reward)
                episode_lengths.append(steps)
                episode_successes.append(current_success)

                # ===== å…³é”®æ”¹åŠ¨ï¼šåˆ†åˆ«å­˜å‚¨å…¨éƒ¨æ•°æ®å’ŒæˆåŠŸæ•°æ® =====
                all_data.extend(episode_data)  # æ‰€æœ‰æ•°æ®éƒ½åŠ å…¥

                if current_success:  # åªæœ‰æˆåŠŸçš„episodeæ‰åŠ å…¥æˆåŠŸæ•°æ®
                    success_data.extend(episode_data)

                if (i + 1) % 5 == 0:
                    success_symbol = "âœ“" if current_success else "âœ—"
                    print(f"  Episode {i+1}/{num_episodes}: Reward={episode_reward:.2f}, Steps={steps}, Success={success_symbol}")

        policy.train()
        self.model.train()  # ç¡®ä¿ä¸»æ¨¡å‹ä¹Ÿå›åˆ°è®­ç»ƒæ¨¡å¼

        # è¿”å›æ•°æ®å’Œç»Ÿè®¡
        stats = {
            'mean_reward': np.mean(episode_rewards),
            'std_reward': np.std(episode_rewards),
            'min_reward': np.min(episode_rewards),
            'max_reward': np.max(episode_rewards),
            'mean_length': np.mean(episode_lengths),
            'success_rate': np.mean(episode_successes),
            'num_successes': int(np.sum(episode_successes))
        }

        # æ‰“å°è¯¦ç»†ç»Ÿè®¡
        print(f"  ğŸ“Š Rolloutç»Ÿè®¡: æˆåŠŸç‡={stats['success_rate']:.1%} ({stats['num_successes']}/{num_episodes}), "
              f"å¥–åŠ±={stats['mean_reward']:.1f}Â±{stats['std_reward']:.1f}")
        print(f"  ğŸ“¦ æ•°æ®é‡: å…¨éƒ¨={len(all_data)}æ­¥, æˆåŠŸ={len(success_data)}æ­¥")

        return all_data, success_data, stats

    def evaluate_policy(self):
        """ä½¿ç”¨env_runnerè¿›è¡Œå‘¨æœŸæ€§è¯„ä¼°"""
        if self.eval_runner is None:
            print("  âš ï¸  æœªé…ç½®env_runnerï¼Œè·³è¿‡è¯„ä¼°")
            return {'success_rate': 0.0}

        print("  è¿è¡Œè¯„ä¼°...")

        # ===== ä½¿ç”¨ EMA æ¨¡å‹è¿›è¡Œè¯„ä¼°ï¼ˆå¦‚æœå¯ç”¨ï¼‰=====
        policy = self.ema_model if self.ema_model is not None else self.model
        policy.eval()

        try:
            eval_results = self.eval_runner.run(policy)
            # ä¿®å¤ï¼šæ­£ç¡®çš„é”®åæ˜¯'test_mean_score'ï¼Œä¸æ˜¯'test/mean_score'
            success_rate = eval_results.get('test_mean_score', 0.0)
            mean_reward = eval_results.get('mean_traj_rewards', 0.0)

            # è®°å½•åˆ°WandB
            wandb.log({
                'eval/success_rate': success_rate,
                'eval/mean_reward': mean_reward,
                'eval/SR_test_L3': eval_results.get('SR_test_L3', 0.0),
                'eval/SR_test_L5': eval_results.get('SR_test_L5', 0.0),
            }, step=self.global_step)

            print(f"  âœ“ è¯„ä¼°å®Œæˆ - æˆåŠŸç‡: {success_rate:.2%}, å¹³å‡å¥–åŠ±: {mean_reward:.2f}")

            policy.train()
            self.model.train()  # ç¡®ä¿ä¸»æ¨¡å‹ä¹Ÿå›åˆ°è®­ç»ƒæ¨¡å¼
            return {'success_rate': success_rate, 'mean_reward': mean_reward}

        except Exception as e:
            print(f"  âš ï¸  è¯„ä¼°å¤±è´¥: {e}")
            import traceback
            traceback.print_exc()
            policy.train()
            self.model.train()
            return {'success_rate': 0.0, 'mean_reward': 0.0}

    def save_checkpoint(self, filename):
        """ä¿å­˜checkpoint"""
        path = os.path.join(self.output_dir, "checkpoints2", filename)
        os.makedirs(os.path.dirname(path), exist_ok=True)

        checkpoint = {
            'model_state_dict': self.model.state_dict(),
            'optimizer_state_dict': self.optimizer.state_dict(),
            'normalizer_state_dict': self.normalizer.state_dict(),
            'global_step': self.global_step,
            'best_success_rate': self.best_success_rate,
            'config': self.cfg,
        }

        # ===== ä¿å­˜ EMA æ¨¡å‹çŠ¶æ€ =====
        if self.ema_model is not None:
            checkpoint['ema_model_state_dict'] = self.ema_model.state_dict()

        if self.lr_scheduler is not None:
            checkpoint['lr_scheduler_state_dict'] = self.lr_scheduler.state_dict()

        torch.save(checkpoint, path)
        print(f"  ğŸ’¾ Checkpointå·²ä¿å­˜: {path}")

    def load_checkpoint(self, path):
        """åŠ è½½checkpoint"""
        print(f"  ğŸ“‚ åŠ è½½checkpoint: {path}")
        payload = torch.load(path, map_location=self.device)

        self.model.load_state_dict(payload['model_state_dict'])
        self.optimizer.load_state_dict(payload['optimizer_state_dict'])

        if 'normalizer_state_dict' in payload:
            self.normalizer.load_state_dict(payload['normalizer_state_dict'])

        # ===== åŠ è½½ EMA æ¨¡å‹çŠ¶æ€ =====
        if self.ema_model is not None and 'ema_model_state_dict' in payload:
            self.ema_model.load_state_dict(payload['ema_model_state_dict'])
            print("  âœ“ EMA æ¨¡å‹çŠ¶æ€å·²æ¢å¤")

        self.global_step = payload.get('global_step', 0)
        self.best_success_rate = payload.get('best_success_rate', 0.0)

        if self.lr_scheduler is not None and 'lr_scheduler_state_dict' in payload:
            self.lr_scheduler.load_state_dict(payload['lr_scheduler_state_dict'])

        print(f"  âœ“ æ¢å¤è®­ç»ƒ - Step: {self.global_step}, æœ€ä½³æˆåŠŸç‡: {self.best_success_rate:.2%}")

    def freeze_encoder(self):
        """å†»ç»“obs_encoderçš„æ‰€æœ‰å‚æ•°"""
        for param in self.model.obs_encoder.parameters():
            param.requires_grad = False
        print("[Info] Encoder has been frozen. No gradients will be computed for encoder in subsequent RL phases.")


@hydra.main(config_path="diffusion_policy_3d/config", config_name="train_rl100", version_base=None)
def main(cfg):
    workspace = TrainRL100Improved(cfg)
    workspace.run()


if __name__ == "__main__":
    main()
